# ğŸš€ Spark Data Processing Pipeline

## ğŸ“‹ Overview
This project implements an automated data processing pipeline that retrieves CSV files from Amazon S3, validates their schema against required columns, processes them, and loads the data into a MySQL database. The pipeline handles error scenarios by moving invalid files to a designated error directory both locally and in S3.

## âœ¨ Features
- ğŸ“¦ S3 file listing and downloading
- ğŸ” Schema validation against mandatory columns
- âš ï¸ Error handling for files with missing required columns
- â• Additional column handling via concatenation into a single field
- ğŸ—„ï¸ MySQL database integration for tracking file processing status
- ğŸ“ Logging of all operations for audit and troubleshooting
- ğŸ” Secure AWS credential handling with encryption/decryption

## ğŸ”§ Prerequisites
- ğŸ Python 3.6+
- â˜ï¸ AWS account with S3 access
- ğŸ¬ MySQL database
- ğŸ“š Required Python packages (see Installation section)

## ğŸ”Œ Installation

### Clone the repository
```bash
git clone <repository-url>
cd <repository-directory>
```

### Install dependencies
```bash
pip install -r requirements.txt
```

### Set up configuration
Edit the `resources/dev/config.py` file to include your specific settings:
- AWS credentials (encrypted)
- S3 bucket and directory paths
- Local directories for processing
- MySQL database connection details
- Mandatory columns for data validation

## ğŸ“‚ Project Structure
```
my_project/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ readme.md
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ qa/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ prod/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â”œâ”€â”€ sql_scripts/
â”‚   â”‚    â””â”€â”€ table_scripts.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚    â”œâ”€â”€ __init__.py
â”‚   â”‚    â””â”€â”€ delete/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_delete.py
â”‚   â”‚    â”‚      â”œâ”€â”€ database_delete.py
â”‚   â”‚    â”‚      â””â”€â”€ local_file_delete.py
â”‚   â”‚    â””â”€â”€ download/
â”‚   â”‚    â”‚      â””â”€â”€ aws_file_download.py
â”‚   â”‚    â””â”€â”€ move/
â”‚   â”‚    â”‚      â””â”€â”€ move_files.py
â”‚   â”‚    â””â”€â”€ read/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_read.py
â”‚   â”‚    â”‚      â””â”€â”€ database_read.py
â”‚   â”‚    â””â”€â”€ transformations/
â”‚   â”‚    â”‚      â””â”€â”€ jobs/
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ customer_mart_sql_transform_write.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ dimension_tables_join.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ main.py
â”‚   â”‚    â”‚      â”‚     â””â”€â”€sales_mart_sql_transform_write.py
â”‚   â”‚    â””â”€â”€ upload/
â”‚   â”‚    â”‚      â””â”€â”€ upload_to_s3.py
â”‚   â”‚    â””â”€â”€ utility/
â”‚   â”‚    â”‚      â”œâ”€â”€ encrypt_decrypt.py
â”‚   â”‚    â”‚      â”œâ”€â”€ logging_config.py
â”‚   â”‚    â”‚      â”œâ”€â”€ s3_client_object.py
â”‚   â”‚    â”‚      â”œâ”€â”€ spark_session.py
â”‚   â”‚    â”‚      â””â”€â”€ my_sql_session.py
â”‚   â”‚    â””â”€â”€ write/
â”‚   â”‚    â”‚      â”œâ”€â”€ database_write.py
â”‚   â”‚    â”‚      â””â”€â”€ parquet_write.py
â”‚   â”œâ”€â”€ test/
â”‚   â”‚    â”œâ”€â”€ scratch_pad.py.py
â”‚   â”‚    â””â”€â”€ generate_csv_data.py
```

```

## âš™ï¸ Configuration
The application is configured through the `resources/dev/config.py` file. Key configuration parameters include:

| Parameter | Description |
|-----------|-------------|
| aws_access_key | Encrypted AWS access key |
| aws_secret_key | Encrypted AWS secret key |
| bucket_name | S3 bucket name |
| s3_source_directory | Source directory path in S3 |
| s3_error_directory | Error directory path in S3 |
| local_directory | Local directory for processing |
| error_folder_path_local | Local directory for error files |
| database_name | MySQL database name |
| table_name | MySQL table for tracking processing |
| product_staging_table | MySQL staging table |
| mandatory_columns | Required columns for CSV validation |

## ğŸ”„ Workflow

1. **ğŸ”‘ Authentication & Setup**: 
   - Decrypt AWS credentials and establish S3 client connection
   - Verify previous run status by checking MySQL database

2. **ğŸ“¥ File Retrieval**:
   - List files in the S3 source directory
   - Download files to the local directory

3. **ğŸ” Schema Validation**:
   - Analyze each CSV file using Spark
   - Compare schema against mandatory columns
   - Move files with missing columns to error directories

4. **âš¡ Data Processing**:
   - Handle extra columns by concatenating them into a single column
   - Create a unified dataframe with all valid data

5. **ğŸ’¾ Database Integration**:
   - Update MySQL staging table with processing status
   - Track file processing status and timestamps

## ğŸ›¡ï¸ Error Handling
The pipeline includes robust error handling:
- Files with missing required columns are moved to an error directory
- Detailed logging of all errors and operations
- Database tracking of file processing status
- Graceful exit with meaningful error messages

## ğŸ“Š Logging
Comprehensive logging is implemented throughout the application to facilitate debugging and monitoring:
- File operations
- Schema validation
- Database operations
- Error conditions
- Processing status

## ğŸ”’ Security
- AWS credentials are encrypted in the configuration file
- Decryption happens at runtime

## ğŸ”¥ Spark Data Transformation

The pipeline leverages Apache Spark for high-performance data processing, supporting complex transformations on sales and customer data:

### Current Capabilities
- Schema validation of source CSV files
- Handling of missing and extra columns
- Unification of validated data into a single processing dataframe

### Planned Spark Transformations
- **ğŸ’° Sales Incentive Calculations**: Computing performance-based incentives for sales personnel based on sales metrics
- **ğŸ‘¥ Customer Analytics**: Segmentation and analysis of customer purchasing patterns
- **ğŸ“ˆ Dynamic Aggregations**: Period-over-period performance comparisons at various levels (product, store, salesperson)
- **ğŸ”„ Data Enrichment**: Joining sales data with additional datasets for enhanced analytics
- **âš¡ Performance Optimization**: Utilizing Spark's parallelism for processing large volumes of sales data efficiently

### Implementation Details
The pipeline uses PySpark's DataFrame API to:
- Load and validate CSV data with defined schemas
- Apply complex transformations using SQL and DataFrame operations
- Handle various data types including numeric, string, and date fields
- Support both batch processing and potential future streaming capabilities

These Spark transformations turn raw sales data into actionable business intelligence, enabling data-driven decision making for sales incentive programs and customer relationship management.

## ğŸ”§ Extending the Project
To add new functionality:
1. Extend the schema validation for different file types
2. Add more data transformation steps
3. Implement additional destination options beyond MySQL
4. Create a monitoring dashboard for the pipeline

## ğŸ” Troubleshooting
- Check logs for detailed error information
- Verify AWS credentials and permissions
- Ensure MySQL connection details are correct
- Validate that the S3 paths and bucket names are properly configured
